{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable, Function\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 三个步骤：定义模型，梯度回传，更新模型\n",
    "## 1. 定义模型有两种方法\n",
    "D_in, H, D_out = 100, 10, 100\n",
    "model1 = nn.Sequential(\n",
    "    nn.Linear(D_in, H),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "class CModel(nn.Module):\n",
    "    def __init__(self, d_in, h, d_out):\n",
    "        super(CModel, self).__init__()\n",
    "        self.input_layer = nn.Linear(d_in, h)\n",
    "        self.hidden = nn.ReLU()\n",
    "        self.output = nn.Linear(h, d_out)\n",
    "    def forward(self, x_in):\n",
    "        x = self.input_layer(x_in)\n",
    "        x = self.hidden(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "    \n",
    "model2 = CModel(D_in, H, D_out)\n",
    "model2.parameters = model1.parameters\n",
    "# prepare data\n",
    "x = torch.randn((10, 100))\n",
    "x1 = Variable(x, requires_grad=True).clone()\n",
    "x2 = Variable(x, requires_grad=True).clone()\n",
    "y_pred_1 = model1(x1)\n",
    "y_pred_2 = model2(x2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 图结构控制：动态多层和参数共享\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we construct three nn.Linear instances that we will use\n",
    "        in the forward pass.\n",
    "        \"\"\"\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        For the forward pass of the model, we randomly choose either 0, 1, 2, or 3\n",
    "        and reuse the middle_linear Module that many times to compute hidden layer\n",
    "        representations.\n",
    "\n",
    "        Since each forward pass builds a dynamic computation graph, we can use normal\n",
    "        Python control-flow operators like loops or conditional statements when\n",
    "        defining the forward pass of the model.\n",
    "\n",
    "        Here we also see that it is perfectly safe to reuse the same Module many\n",
    "        times when defining a computational graph. This is a big improvement from Lua\n",
    "        Torch, where each Module could be used only once.\n",
    "        \"\"\"\n",
    "        h_relu = self.input_linear(x).clamp(min=0)\n",
    "        for _ in range(random.randint(0, 3)):\n",
    "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        return y_pred\n",
    "    \n",
    "# PS: 这里，中间线性层进行了多次迭代（迭代次数由随机数决定），并且进行了参数共享（中间层的重复，用的是同一个参数矩阵）"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}